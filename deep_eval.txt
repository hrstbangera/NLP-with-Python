from deepeval.models import DeepEvalBaseLLM
from langchain_community.llms import Ollama  # Import the Ollama model

class CustomOllamaModel(DeepEvalBaseLLM):
    def __init__(self):
        # Initialize the Ollama model
        self.model = Ollama(model="gemma2")  # Replace with the desired Ollama model name
        self.tokenizer = None  # Ollama models don't require a separate tokenizer

    def load_model(self):
        return self.model

    def generate(self, prompt: str) -> str:
        model = self.load_model()
        
        # Generate text using the Ollama model
        response = model(prompt)  # Assuming `model` is callable and returns the response text

        return response

    async def a_generate(self, prompt: str) -> str:
        # Use the same synchronous generate method for now
        return self.generate(prompt)

    def get_model_name(self):
        return "Ollama-Gemma2"  # Replace with the appropriate Ollama model name

# Example usage
custom_llm = CustomOllamaModel()
print(custom_llm.generate("Write me a joke"))

from deepeval.metrics import AnswerRelevancyMetric

metric = AnswerRelevancyMetric(model=custom_llm)
# metric.measure(...)  # Use this with appropriate input for evaluation
---------------------

from deepeval.scorer import Scorer
from deepeval.metrics import BaseMetric
from deepeval.test_case import LLMTestCase

class RougeMetric(BaseMetric):
    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
        self.scorer = Scorer()  # Initialize the Scorer here
        self.success = False  # Initialize self.success here

    def measure(self, test_case: LLMTestCase):
        # Measure the Rouge score using the scorer object
        self.score = self.scorer.rouge_score(
            prediction=test_case.actual_output,
            target=test_case.expected_output,
            score_type="rouge1"
        )
        self.success = self.score >= self.threshold
        return self.score

    async def a_measure(self, test_case: LLMTestCase):
        return self.measure(test_case)

    def is_successful(self):
        return self.success

    @property
    def __name__(self):
        return "Rouge Metric"

# Example usage
test_case = LLMTestCase(input="What is AI?", actual_output="AI stands for Artificial Intelligence.", expected_output="AI is short for Artificial Intelligence.")
metric = RougeMetric()

metric.measure(test_case)
print(metric.is_successful())
-----------

json error --code

import json
from pydantic import BaseModel
from lmformatenforcer import JsonSchemaParser
from lmformatenforcer.integrations.transformers import (
    build_transformers_prefix_allowed_tokens_fn,
)

from deepeval.models import DeepEvalBaseLLM
from langchain_community.llms import Ollama  # Import the Ollama model


class CustomOllamaModel(DeepEvalBaseLLM):
    def __init__(self, model_name: str = "gemma2", temperature: float = 1.0):
        # Initialize the Ollama model with a flexible model name and temperature control
        self.model_name = model_name
        self.model = Ollama(model=model_name)  # Use the provided model name
        self.tokenizer = None  # Ollama models don't require a separate tokenizer
        self.temperature = temperature  # Store the temperature value

    def load_model(self):
        return self.model

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        model = self.load_model()

        # Create parser required for JSON confinement using lmformatenforcer
        parser = JsonSchemaParser(schema.schema())
        prefix_function = build_transformers_prefix_allowed_tokens_fn(
            model.tokenizer, parser
        )

        # Generate text using the Ollama model
        response = model(prompt, temperature=self.temperature)

        # Ensure the generated text is valid JSON
        try:
            output = response[len(prompt):]
            json_result = json.loads(output)
        except json.JSONDecodeError:
            raise ValueError("Invalid JSON generated by the model.")

        # Return valid JSON object according to the schema DeepEval supplied
        return schema(**json_result)

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        return self.generate(prompt, schema)

    def get_model_name(self):
        return self.model_name  # Return the model name

# Example usage with a custom model name and temperature
# Assuming you have a Pydantic schema defined, you would pass it like this:
# custom_llm = CustomOllamaModel(model_name="gemma2", temperature=0.7)
# result = custom_llm.generate("Write me a joke", MyPydanticSchema)
# print(result)



