from langchain.llms import LLM
from langchain_openai import AzureChatOpenAI
from deepeval.models.base_model import DeepEvalBaseLLM

class AzureOpenAI(DeepEvalBaseLLM, LLM):
    def __init__(
        self,
        model
    ):
        self.model = model

    def load_model(self):
        return self.model

    def generate(self, prompt: str) -> str:
        """Generate a response from the model using the given prompt."""
        chat_model = self.load_model()
        # For LangChain LLM compatibility, return the text response
        return chat_model.invoke(prompt).content

    async def a_generate(self, prompt: str) -> str:
        """Asynchronously generate a response from the model."""
        chat_model = self.load_model()
        res = await chat_model.ainvoke(prompt)
        return res.content

    def get_model_name(self):
        """Return the name of the custom Azure OpenAI model."""
        return "Custom Azure OpenAI Model"

    @property
    def _llm_type(self) -> str:
        """Define the type of LLM."""
        return "azure_openai"

    def _call(self, prompt: str, stop: list = None) -> str:
        """Method that LangChain uses to generate responses."""
        return self.generate(prompt)

    async def _acall(self, prompt: str, stop: list = None) -> str:
        """Asynchronous method that LangChain uses to generate responses."""
        return await self.a_generate(prompt)

# Replace these with real values
custom_model = AzureChatOpenAI(
    openai_api_version=openai_api_version,
    azure_deployment=azure_deployment,
    azure_endpoint=azure_endpoint,
    openai_api_key=openai_api_key,
)
azure_openai = AzureOpenAI(model=custom_model)

# Use the LLM interface's call method for generation
print(azure_openai("Write me a joke"))
