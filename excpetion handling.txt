from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

class DocumentProcessingError(Exception):
    """
    Custom exception class for document processing errors.
    """
    def __init__(self, message: str):
        super().__init__(message)

class DocumentLoaderError(DocumentProcessingError):
    """
    Raised when there is an issue loading documents.
    """
    pass

class VectorStoreError(DocumentProcessingError):
    """
    Raised when there is an issue with the vector store.
    """
    pass

class RetrieverError(DocumentProcessingError):
    """
    Raised when there is an issue adding documents to the retriever.
    """
    pass

try:
    # Load documents
    loaders = [
        TextLoader("paul_graham_essay.txt"),
        TextLoader("state_of_the_union.txt"),
    ]
    docs = []
    for loader in loaders:
        try:
            docs.extend(loader.load())
        except FileNotFoundError as fnf_error:
            raise DocumentLoaderError(f"File not found: {fnf_error}")
        except Exception as e:
            raise DocumentLoaderError(f"Error loading document: {e}")

    if not docs:
        raise DocumentLoaderError("No documents loaded. Please check your file paths.")

    # This text splitter is used to create the child documents
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

    # The vectorstore to use to index the child chunks
    try:
        vectorstore = Chroma(
            collection_name="full_documents", embedding_function=OpenAIEmbeddings()
        )
    except Exception as e:
        raise VectorStoreError(f"Error initializing vector store: {e}")

    # The storage layer for the parent documents
    store = InMemoryStore()

    try:
        retriever = ParentDocumentRetriever(
            vectorstore=vectorstore,
            docstore=store,
            child_splitter=child_splitter,
        )

        # Attempt to add documents
        retriever.add_documents(docs, ids=None)
        print("Documents successfully added to the retriever.")

    except Exception as e:
        raise RetrieverError(f"Error adding documents to retriever: {e}")

except DocumentProcessingError as dpe:
    print(f"DocumentProcessingError: {dpe}")

except Exception as e:
    print(f"An unexpected error occurred: {e}")
---------------------------------------------------------------

from fastapi import HTTPException

class DocumentRetrievalError(Exception):
    """Raised when document retrieval fails."""
    pass

class StreamingResponseError(Exception):
    """Raised during streaming response issues."""
    pass

from fastapi import Request
from fastapi.responses import JSONResponse

@app.exception_handler(DocumentRetrievalError)
async def document_retrieval_error_handler(request: Request, exc: DocumentRetrievalError):
    return JSONResponse(
        status_code=404,
        content={"error": "Relevant documents could not be retrieved from Chroma DB."},
    )

@app.exception_handler(StreamingResponseError)
async def streaming_error_handler(request: Request, exc: StreamingResponseError):
    return JSONResponse(
        status_code=500,
        content={"error": "Error while generating streaming response."},
    )


from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
import json

app = FastAPI()

@app.post("/stream")
async def stream(request: Request):
    try:
        # Parse user input
        inputs = await request.json()
        user_input = inputs["inputs"]["question"]

        # Fetch relevant documents from Chroma DB
        if not user_input:
            raise DocumentRetrievalError("No user input provided for document retrieval.")

        ### Placeholder for your actual logic ###
        stream = []  # Replace with actual stream generator logic
        
        # Streaming generator for RAG response
        async def rai_stream_gen(stream):
            async for chunk in stream:
                yield (chunk, False, "No source, this is streaming")
            yield ("", True, "Unique documents")

        # Format streaming generator output
        async def formatted_stream_gen(gen):
            async for items in gen:
                yield json.dumps(items)

        return StreamingResponse(
            formatted_stream_gen(rai_stream_gen(stream)),
            media_type="text/explain"
        )

    except DocumentRetrievalError as dre:
        # Map custom exception to a response
        raise HTTPException(status_code=404, detail=str(dre))

    except KeyError as ke:
        raise HTTPException(status_code=400, detail="Malformed input. Missing required keys.")

    except Exception as e:
        raise StreamingResponseError(f"Unexpected error: {e}")

