import subprocess
import time
import os
import signal

# Step 1: Start backend and frontend using subprocess
backend_process = subprocess.Popen(["python", "server/local_server.py"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
frontend_process = subprocess.Popen(["streamlit", "run", "streamlit/ui.py", "--server.port", "4550"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

print(f"Backend PID: {backend_process.pid}, Frontend PID: {frontend_process.pid}")

# Step 2: Wait for 1 hour (3600 seconds)
time.sleep(3600)

# Step 3: Kill both backend and frontend processes
os.kill(backend_process.pid, signal.SIGTERM)  # Terminate backend
os.kill(frontend_process.pid, signal.SIGTERM)  # Terminate frontend

print("Both backend and frontend processes have been terminated.")
---------------

import subprocess
import time
import os
import signal
import threading

def start_backend():
    global backend_process
    backend_process = subprocess.Popen(["python", "server/local_server.py"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print(f"Backend started with PID: {backend_process.pid}")

def start_frontend():
    global frontend_process
    frontend_process = subprocess.Popen(["streamlit", "run", "streamlit/ui.py", "--server.port", "4550"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print(f"Frontend started with PID: {frontend_process.pid}")

def stop_processes_after_delay(delay):
    time.sleep(delay)
    os.kill(backend_process.pid, signal.SIGTERM)  # Terminate backend
    os.kill(frontend_process.pid, signal.SIGTERM)  # Terminate frontend
    print("Both backend and frontend processes have been terminated.")

# Start backend and frontend in separate threads
backend_thread = threading.Thread(target=start_backend)
frontend_thread = threading.Thread(target=start_frontend)

backend_thread.start()
frontend_thread.start()

# Wait for both threads to start processes
backend_thread.join()
frontend_thread.join()

# Schedule the stop after 1 hour
stop_thread = threading.Thread(target=stop_processes_after_delay, args=(3600,))
stop_thread.start()

------------------

import psutil

def kill_process_using_port(port):
    for proc in psutil.process_iter(['pid', 'name', 'connections']):
        for conn in proc.info['connections']:
            if conn.laddr.port == port:
                print(f"Killing process {proc.info['name']} with PID {proc.info['pid']} on port {port}")
                proc.kill()

kill_process_using_port(4550)
---------------


import ollama

# Initialize the Ollama model
model = ollama.Ollama('gemma2')

# Define the two questions for comparison
q1 = "How do I support my spouse during a difficult time?"
q2 = "How do I help my partner through a tough time?"

# Create the few-shot prompt with examples
prompt = f"""
Given two questions, compare their similarity based on content and intent.

Treat questions as similar if they refer to the same relationship (e.g., spousal relationship) but use different wording.
Do not match questions where the roles change or imply different responsibilities (e.g., 'manager' vs. 'employee').

Provide the result in JSON format with two fields:
- "match": true or false
- "reason": a brief explanation for why the questions are matched or not.

### Example 1:
Question 1: "How do I support my spouse during a difficult time?"
Question 2: "How do I help my partner through a tough time?"

Output:
{{
  "match": true,
  "reason": "Both questions refer to supporting a spouse/partner during a difficult time, using different wording but sharing the same intent."
}}

### Example 2:
Question 1: "What are my responsibilities as a team lead?"
Question 2: "What are the tasks of a team member?"

Output:
{{
  "match": false,
  "reason": "The first question refers to a team lead's responsibilities, while the second refers to a team member's tasks, which implies different roles."
}}

### Now, compare the following questions:

Question 1: "{q1}"
Question 2: "{q2}"

Output the result in the same JSON format.
"""

# Generate a response from the model
response = model.generate(prompt=prompt)

# Output the model's response
print(response['text'])
-----------------------------
from typing import List
from deepeval.models import DeepEvalBaseEmbeddingModel
from langchain_community.llms import Ollama  # Assuming you're using the Langchain Ollama wrapper

class CustomEmbeddingModel(DeepEvalBaseEmbeddingModel):
    def __init__(self):
        pass

    def load_model(self):
        # Loading Ollama model (e.g., 'gemma2' or any other you plan to use for embeddings)
        return Ollama(model="gemma2")

    def embed_text(self, text: str) -> List[float]:
        embedding_model = self.load_model()
        # Assuming Ollama supports embedding text, adapt the method here
        return embedding_model.embed_text(text)

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        embedding_model = self.load_model()
        # Embedding multiple texts
        return [embedding_model.embed_text(text) for text in texts]

    async def a_embed_text(self, text: str) -> List[float]:
        embedding_model = self.load_model()
        # If Ollama supports async embeddings
        return await embedding_model.aembed_text(text)

    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:
        embedding_model = self.load_model()
        return await [embedding_model.aembed_text(text) for text in texts]

    def get_model_name(self):
        return "Custom Ollama Embedding Model"
----------------
def process_query(user_query):
    appendix_details = {
        'tier 1': 'Details about tier 1...',
        'tier 2': 'Details about tier 2...',
        'tier 3': 'Details about tier 3...',
        'approved brokers list': 'List of approved brokers...'
    }

    prompt = user_query  # Start with the user query as the base prompt

    # Check for specific keywords and append appendix details
    for keyword, details in appendix_details.items():
        if keyword.lower() in user_query.lower():
            prompt += f"\n\n{details}"

    return prompt

# Example usage:
user_query = "What is the process for tier 1 suppliers?"
final_prompt = process_query(user_query)
print(final_prompt)
-------------
def process_query(user_query):
    # Define appendix details for policy tiers and brokerage
    appendix_details = {
        'policy_tiers': 'Details about policy tiers...',
        'brokerage': 'Details about brokerage accounts and approved brokers...'
    }

    # Define lists of keywords for policy tiers and brokerage
    policy_tier_keywords = [
        'tier 1', 'tier 2', 'tier 3', 'tier1', 'tier2', 'tier3', 'tiers', 'policy tiers'
    ]
    brokerage_keywords = [
        'broker', 'brokerage accounts', 'approved brokers', 'brokerage'
    ]

    prompt = user_query  # Start with the user query as the base prompt
    added_details = False  # Track if any details are added

    # Check if the query contains any policy tier keywords
    for keyword in policy_tier_keywords:
        if keyword.lower() in user_query.lower():
            prompt += f"\n\n{appendix_details['policy_tiers']}"
            added_details = True
            break  # Append policy tier details once and exit loop

    # Check if the query contains any brokerage-related keywords
    for keyword in brokerage_keywords:
        if keyword.lower() in user_query.lower():
            prompt += f"\n\n{appendix_details['brokerage']}"
            added_details = True
            break  # Append brokerage details once and exit loop

    # Return the modified prompt, or original if no relevant keywords found
    return prompt if added_details else user_query

# Example usage:
user_query = "Can you explain the process for tier 1 and approved brokers?"
final_prompt = process_query(user_query)
print(final_prompt)
----------------------

import ollama

# Define the function tool for question comparison
compare_questions_tool = {
    'type': 'function',
    'function': {
        'name': 'compare_questions',
        'description': 'Compare two questions to check if they match in terms of content and intent.',
        'parameters': {
            'type': 'object',
            'properties': {
                'question1': {
                    'type': 'string',
                    'description': 'The first question to compare',
                },
                'question2': {
                    'type': 'string',
                    'description': 'The second question to compare',
                },
            },
            'required': ['question1', 'question2'],
        },
    }
}

# Define the two questions for comparison
q1 = "How do I support my spouse during a difficult time?"
q2 = "How do I help my partner through a tough time?"

# Create the Ollama chat request with tool calling
response = ollama.chat(
    model='llama3.1',
    messages=[{'role': 'user', 'content': f'Compare the following questions:\nQuestion 1: {q1}\nQuestion 2: {q2}'}],
    tools=[compare_questions_tool]
)

# Output the response and any tool calls
print(response['message']['tool_calls'])

