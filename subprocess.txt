import subprocess
import time
import os
import signal

# Step 1: Start backend and frontend using subprocess
backend_process = subprocess.Popen(["python", "server/local_server.py"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
frontend_process = subprocess.Popen(["streamlit", "run", "streamlit/ui.py", "--server.port", "4550"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

print(f"Backend PID: {backend_process.pid}, Frontend PID: {frontend_process.pid}")

# Step 2: Wait for 1 hour (3600 seconds)
time.sleep(3600)

# Step 3: Kill both backend and frontend processes
os.kill(backend_process.pid, signal.SIGTERM)  # Terminate backend
os.kill(frontend_process.pid, signal.SIGTERM)  # Terminate frontend

print("Both backend and frontend processes have been terminated.")
---------------

import subprocess
import time
import os
import signal
import threading

def start_backend():
    global backend_process
    backend_process = subprocess.Popen(["python", "server/local_server.py"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print(f"Backend started with PID: {backend_process.pid}")

def start_frontend():
    global frontend_process
    frontend_process = subprocess.Popen(["streamlit", "run", "streamlit/ui.py", "--server.port", "4550"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print(f"Frontend started with PID: {frontend_process.pid}")

def stop_processes_after_delay(delay):
    time.sleep(delay)
    os.kill(backend_process.pid, signal.SIGTERM)  # Terminate backend
    os.kill(frontend_process.pid, signal.SIGTERM)  # Terminate frontend
    print("Both backend and frontend processes have been terminated.")

# Start backend and frontend in separate threads
backend_thread = threading.Thread(target=start_backend)
frontend_thread = threading.Thread(target=start_frontend)

backend_thread.start()
frontend_thread.start()

# Wait for both threads to start processes
backend_thread.join()
frontend_thread.join()

# Schedule the stop after 1 hour
stop_thread = threading.Thread(target=stop_processes_after_delay, args=(3600,))
stop_thread.start()

------------------

import psutil

def kill_process_using_port(port):
    for proc in psutil.process_iter(['pid', 'name', 'connections']):
        for conn in proc.info['connections']:
            if conn.laddr.port == port:
                print(f"Killing process {proc.info['name']} with PID {proc.info['pid']} on port {port}")
                proc.kill()

kill_process_using_port(4550)
---------------


import ollama

# Initialize the Ollama model
model = ollama.Ollama('gemma2')

# Define the two questions for comparison
q1 = "How do I support my spouse during a difficult time?"
q2 = "How do I help my partner through a tough time?"

# Create the few-shot prompt with examples
prompt = f"""
Given two questions, compare their similarity based on content and intent.

Treat questions as similar if they refer to the same relationship (e.g., spousal relationship) but use different wording.
Do not match questions where the roles change or imply different responsibilities (e.g., 'manager' vs. 'employee').

Provide the result in JSON format with two fields:
- "match": true or false
- "reason": a brief explanation for why the questions are matched or not.

### Example 1:
Question 1: "How do I support my spouse during a difficult time?"
Question 2: "How do I help my partner through a tough time?"

Output:
{{
  "match": true,
  "reason": "Both questions refer to supporting a spouse/partner during a difficult time, using different wording but sharing the same intent."
}}

### Example 2:
Question 1: "What are my responsibilities as a team lead?"
Question 2: "What are the tasks of a team member?"

Output:
{{
  "match": false,
  "reason": "The first question refers to a team lead's responsibilities, while the second refers to a team member's tasks, which implies different roles."
}}

### Now, compare the following questions:

Question 1: "{q1}"
Question 2: "{q2}"

Output the result in the same JSON format.
"""

# Generate a response from the model
response = model.generate(prompt=prompt)

# Output the model's response
print(response['text'])

