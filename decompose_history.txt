from openai import OpenAI
import json

client = OpenAI(api_key="your-api-key")

# Tool schema definition
rag_tool = {
    "type": "function",
    "function": {
        "name": "prepare_rag_query",
        "description": "Prepares user query for a RAG system by either decomposing, rephrasing using chat history, or returning it as-is.",
        "parameters": {
            "type": "object",
            "properties": {
                "user_question": {
                    "type": "string",
                    "description": "The current user question."
                },
                "chat_history": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Previous messages for context, if any."
                }
            },
            "required": ["user_question"]
        }
    }
}

# Example inputs
chat_history = [
    "Tell me about Apple's performance.",
    "Did they grow last year?"
]
user_question = "What about its sales in 2023?"

# Make the function/tool call
response = client.chat.completions.create(
    model="gpt-4-0613",
    messages=[
        {"role": "system", "content": "You are a RAG query optimizer. You decide if a user query should be rephrased using history, decomposed if complex, or returned as-is for retrieval."},
        {"role": "user", "content": f"user_question: {user_question}\nchat_history: {chat_history}"}
    ],
    tools=[rag_tool],
    tool_choice={"type": "function", "function": {"name": "prepare_rag_query"}}
)

# Extract tool call
tool_call = response.choices[0].message.tool_calls[0]
arguments = json.loads(tool_call.function.arguments)

print(f"\nüîß Tool Called: {tool_call.function.name}")
print(f"üßæ Arguments Passed:\n{json.dumps(arguments, indent=2)}")

# Simulated local implementation of the tool
def prepare_rag_query(user_question, chat_history=None):
    chat_history = chat_history or []

    # Example logic (simple heuristic for demo)
    if any(q in user_question.lower() for q in ["compare", "difference", "pros and cons"]):
        return {
            "query_type": "decomposed",
            "queries": [
                "What is Tesla's battery strategy?",
                "What is Toyota's hybrid strategy?",
                "How do they compare?"
            ]
        }
    elif "sales" in user_question.lower() and chat_history:
        return {
            "query_type": "rephrased",
            "queries": ["What were Apple's sales in 2023?"]
        }
    else:
        return {
            "query_type": "as_is",
            "queries": [user_question]
        }

# Call the local function with the LLM-decided arguments
result = prepare_rag_query(**arguments)

# Output the final result
print(f"\n‚úÖ Final Output:")
print(f"Query Type: {result['query_type']}")
for idx, q in enumerate(result["queries"], 1):
    print(f"{idx}. {q}")



--------------------
def prepare_rag_query(user_question, chat_history=None):
    """
    Prepares a RAG-friendly version of a user question.
    Returns:
    - query_type: 'as_is', 'rephrased', or 'decomposed'
    - queries: list of questions
    """
    # Here you might apply additional logic locally if needed
    # But for this example, assume the LLM determines the output
    return {
        "query_type": "as_is",
        "queries": [user_question]
    }


rag_tool_schema = {
    "name": "prepare_rag_query",
    "description": "Analyzes a user query and chat history to prepare optimized questions for document retrieval in a RAG system.",
    "parameters": {
        "type": "object",
        "properties": {
            "user_question": {
                "type": "string",
                "description": "The current user question."
            },
            "chat_history": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of previous chat messages for context."
            }
        },
        "required": ["user_question"]
    }
}

import openai
import json

openai.api_key = OPENAI_API_KEY

response = openai.chat.completions.create(
    model="gpt-4-0613",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that prepares questions for a RAG-based system."},
        {"role": "user", "content": "What about its sales in 2023?"},
    ],
    functions=[rag_tool_schema],
    function_call={"name": "prepare_rag_query"}
)

function_call = response.choices[0].message.function_call
args = json.loads(function_call.arguments)

# Call your local Python function using the extracted arguments
result = prepare_rag_query(args['user_question'], args.get('chat_history', []))

print(f"\nQuery Type: {result['query_type']}")
print("Queries:")
for q in result['queries']:
    print("-", q)

-------------------------
from openai import OpenAI
import json

client = OpenAI(api_key="your-api-key")

# Chat history and user question
chat_history = [
    {"role": "user", "content": "Tell me about Apple's performance."},
    {"role": "assistant", "content": "It was good... sales increased by 10%..."}
]
user_question = "What about its sales in 2023?"

# System prompt
system_prompt = """
You are a RAG query optimizer. Based on the user query and previous conversation, prepare the question for retrieval.

Respond ONLY in the following JSON format:
{
  "query_type": "rephrased" | "decomposed" | "as_is",
  "queries": ["..."]
}

Guidelines:
- If the question is complex or multi-hop, decompose it into smaller queries.
- If the question depends on previous messages, rephrase it as a standalone question.
- If the question is already self-contained, return it as-is.
"""

# Run chat completion
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": system_prompt},
        *chat_history,
        {"role": "user", "content": user_question}
    ],
    temperature=0.2
)

# Parse the response
response_content = response.choices[0].message.content
try:
    result = json.loads(response_content)
    print("‚úÖ Parsed Result:")
    print(json.dumps(result, indent=2))
except Exception as e:
    print("‚ö†Ô∏è Could not parse JSON:", response_content)


